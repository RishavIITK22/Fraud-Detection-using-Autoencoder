# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sv6MVV1Fr9PR_vlQuHDjAWfeazNwve2i
"""

#FETAURE ENGINEERING
combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])
# Remove duplicate merchant IDs
combined_df = combined_df.drop_duplicates(subset=['merchant_id', 'customer_id', 'transaction_id'])
# Group by 'merchant_id'
grouped = combined_df.groupby('merchant_id')
# Initialize an empty DataFrame to hold features
features = pd.DataFrame()

#Transaction based features
# Transactions per Day
transactions_per_day = grouped['timestamp'].apply(lambda x: x.dt.date.nunique()).rename('transactions_per_day')

# Transactions per Week
transactions_per_week = grouped['timestamp'].apply(lambda x: x.dt.isocalendar().week.nunique()).rename('transactions_per_week')

# Transactions per Month
transactions_per_month = grouped['timestamp'].apply(lambda x: x.dt.month.nunique()).rename('transactions_per_month')
# Average Transactions per Hour
def avg_transactions_per_hour(x):
    hourly_counts = x.dt.hour.value_counts(normalize=True).sort_index()
    # To ensure all 24 hours are represented
    hourly_counts = hourly_counts.reindex(range(24), fill_value=0)
    return hourly_counts.values

avg_trans_per_hour = grouped['timestamp'].apply(avg_transactions_per_hour)
# Transform the resulting Series into a DataFrame
avg_trans_per_hour_df = pd.DataFrame(avg_trans_per_hour.tolist(), index=avg_trans_per_hour.index)

# Add appropriate column names for hours
avg_trans_per_hour_df.columns = [f'avg_trans_hour_{hour}' for hour in range(24)]
# Peak Transaction Hours (e.g., top 3 hours)
def peak_transaction_hours(x, top_n=3):
    counts = x.dt.hour.value_counts().nlargest(top_n).index.tolist()
    return ','.join(map(str, counts))

peak_hours = grouped['timestamp'].apply(peak_transaction_hours).rename('peak_transaction_hours')

# Rolling Average Transactions (e.g., 7-day rolling average)
combined_df_sorted = combined_df.sort_values(['merchant_id', 'timestamp'])
# Apply rolling window operation
rolling_avg = (
    combined_df_sorted.groupby('merchant_id')  # Group by merchant
    .rolling('7D', on='timestamp')['transaction_id']  # Use a 7-day window
    .count()  # Count transactions in the 7-day window
    .reset_index(name='rolling_avg_7d')  # Reset index and rename
)
# Ensure default values for rolling averages when data is insufficient
rolling_avg['rolling_avg_7d'] = rolling_avg['rolling_avg_7d'].fillna(0)
# Take the mean of rolling averages per merchant
rolling_avg_features = (
    rolling_avg.groupby('merchant_id')['rolling_avg_7d']
    .mean()
    .rename('rolling_avg_transactions_7d')
)

# Combine all velocity metrics
features = pd.concat([transactions_per_day, transactions_per_week, transactions_per_month,
                      avg_trans_per_hour, rolling_avg_features], axis=1)

#TEMPORAL FEATURES
# Transactions During Odd Hours (e.g., 12 AMâ€“4 AM)
def odd_hours_percentage(x):
    odd_hours = x.dt.hour.between(0, 4)
    return odd_hours.mean()

odd_hours_pct = grouped['timestamp'].apply(odd_hours_percentage).rename('odd_hours_percentage')

# Transactions per Day of Week
transactions_per_dow = grouped['timestamp'].apply(lambda x: x.dt.dayofweek.value_counts(normalize=True)).unstack().fillna(0)
transactions_per_dow.columns = [f'trans_dow_{day}' for day in transactions_per_dow.columns]

# Seasonality: Transactions per Month
transactions_per_month = grouped['timestamp'].apply(lambda x: x.dt.month.value_counts(normalize=True)).unstack().fillna(0)
transactions_per_month.columns = [f'trans_month_{month}' for month in transactions_per_month.columns]

# Combine time-based features
time_based_features = pd.concat([odd_hours_pct, transactions_per_dow, transactions_per_month], axis=1)

#Amount distributions
# Average Transaction Amount
avg_amount = grouped['amount'].mean().rename('avg_transaction_amount')

# Median Transaction Amount
median_amount = grouped['amount'].median().rename('median_transaction_amount')

# Transaction Amount Variance
var_amount = grouped['amount'].var().rename('var_transaction_amount')

# Transaction Amount Standard Deviation
std_amount = grouped['amount'].std().rename('std_transaction_amount')

# Maximum Transaction Amount
max_amount = grouped['amount'].max().rename('max_transaction_amount')

# Minimum Transaction Amount
min_amount = grouped['amount'].min().rename('min_transaction_amount')

# Percentage of High-Amount Transactions (e.g., > 95th percentile)
threshold = combined_df['amount'].quantile(0.95)  # Adjust percentile as needed

high_amount_pct = grouped['amount'].apply(lambda x: (x > threshold).mean()).rename('high_amount_pct')

# Combine amount distribution features
amount_features = pd.concat([avg_amount, median_amount, var_amount, std_amount, max_amount, min_amount, high_amount_pct], axis=1)

#CUSTOMER CONCENTRATION
import numpy as np

# Number of Unique Customers
unique_customers = grouped['customer_id'].nunique().rename('num_unique_customers')

# Gini Coefficient
def gini_coefficient(x):
    # Calculate the Gini coefficient for a distribution of counts
    counts = x.value_counts()
    if len(counts) == 0:
        return 0
    counts = counts.sort_values()
    n = len(counts)
    cumulative_counts = counts.cumsum()
    relative_counts = counts / counts.sum()
    cumulative_rel_counts = cumulative_counts / cumulative_counts.iloc[-1]
    gini = 1 - 2 * np.trapz(cumulative_rel_counts, dx=1/n)
    return gini

gini_customers = grouped['customer_id'].apply(gini_coefficient).rename('gini_customer_distribution')

# Top N Customers' Contribution (e.g., Top 5)
def top_n_customers_pct(x, n=5):
    top_n = x.value_counts().nlargest(n)
    return top_n.sum() / len(x)

top_5_customers_pct = grouped['customer_id'].apply(top_n_customers_pct).rename('top5_customers_pct')

# Combine customer concentration features
customer_concentration_features = pd.concat([unique_customers, gini_customers, top_5_customers_pct], axis=1)

#INTEGRATING ALL FEATURES
# Combine all feature sets
all_features = pd.concat([features, time_based_features, amount_features, customer_concentration_features], axis=1)
# Reset index to have 'merchant_id' as a column
all_features.reset_index(inplace=True)

# Optionally, handle missing values
all_features.fillna(0)
all_merchants = combined_df[['merchant_id']].drop_duplicates()
all_features = all_merchants.merge(all_features, on='merchant_id', how='left')
print(all_features.shape)
# Save the feature matrix to CSV
all_features.to_csv("all_features.csv", index=False)

print("Feature engineering completed and saved to 'merchant_features.csv'.")

#FEATURE NORMALIZATION PIPELINE
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
scaler = StandardScaler()
numeric_features=list(all_features.columns[1:])
numeric_features.remove('timestamp')
# Create a ColumnTransformer to apply the scaler to numeric features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', scaler, numeric_features)
    ])
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])
normalized_data = pipeline.fit_transform(all_features)
normalized_df = pd.DataFrame(normalized_data,columns=numeric_features)
print(normalized_df.head())